{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\anaconda3\\lib\\site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\anaconda3\\lib\\site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\anaconda3\\lib\\site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\anaconda3\\lib\\site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Collecting fitter\n",
      "  Using cached fitter-1.5.2.tar.gz (27 kB)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\lib\\site-packages (from fitter) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\lib\\site-packages (from fitter) (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from fitter) (3.4.3)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from fitter) (1.20.3)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (from fitter) (1.3.4)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\anaconda3\\lib\\site-packages (from fitter) (1.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from fitter) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from click->fitter) (0.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fitter) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fitter) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fitter) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fitter) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->fitter) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->fitter) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\anaconda3\\lib\\site-packages (from pandas->fitter) (2021.3)\n",
      "Building wheels for collected packages: fitter\n",
      "  Building wheel for fitter (setup.py): started\n",
      "  Building wheel for fitter (setup.py): finished with status 'done'\n",
      "  Created wheel for fitter: filename=fitter-1.5.2-py3-none-any.whl size=25610 sha256=4e08223611443b0a9144911f855841f632ba26a37defd9acfad3854ceb2ead8d\n",
      "  Stored in directory: c:\\users\\nszyuz\\appdata\\local\\pip\\cache\\wheels\\2f\\4b\\12\\1c9085f8ecb92805ca8645ab9c61703a2874685a9fb87b0bdb\n",
      "Successfully built fitter\n",
      "Installing collected packages: fitter\n",
      "Successfully installed fitter-1.5.2\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "#Для первого запуска\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install fitter\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт нужных библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from fitter import Fitter\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Выделение итогового набора полей для одной записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Time Parameter      Value\n",
      "0    00:00  RecordID  132539.00\n",
      "1    00:00       Age      54.00\n",
      "2    00:00    Gender       0.00\n",
      "3    00:00    Height      -1.00\n",
      "4    00:00   ICUType       4.00\n",
      "..     ...       ...        ...\n",
      "268  47:37     NIMAP      79.33\n",
      "269  47:37  NISysABP     128.00\n",
      "270  47:37  RespRate      23.00\n",
      "271  47:37      Temp      37.80\n",
      "272  47:37     Urine     280.00\n",
      "\n",
      "[273 rows x 3 columns]\n",
      "RecordID             132539\n",
      "Survival                 -1\n",
      "In-hospital_death         0\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Получение иноформации \"как есть\" для RecordID 132539\n",
    "df_ = pd.read_table('set/132539.txt', delimiter = ',') \n",
    "print(df_)\n",
    "out = pd.read_table('outcomes.txt', delimiter = ',')\n",
    "print(out[['RecordID', 'Survival', 'In-hospital_death']].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все поля представляют собой отдельные значения, разбиение полей с множеством значений на несоклько полей не требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Создание структуры данных для итогового набора полей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ встречаемости параметров: определение, какие параметры встречаются редко (меньше 1 данных на запись в среднем), встречаются ровно один раз в записи, встречаются больше одного раза в записи (для них высчитываем потом среднее значение), встречаются значительно больше одного раза в записи (для них высчитываем потом медианное значение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем все значения RecordID из столбца outcomes.txt\n",
    "out = pd.read_table('outcomes.txt', delimiter = ',')\n",
    "RIDs = out[\"RecordID\"]\n",
    "n = len(RIDs)\n",
    "# Формируем Counter, в котором все параметры будут считаться\n",
    "x = Counter()\n",
    "for rid in RIDs:\n",
    "    df_ = pd.read_table('set/' + str(rid) + '.txt', delimiter = ',') \n",
    "    x = x + Counter(df_[\"Parameter\"])\n",
    "\n",
    "# Разделение параметров по встречаемости\n",
    "unique_parameters = list(x.keys())\n",
    "one_params = []\n",
    "mean_params = []\n",
    "rare_params = []\n",
    "median_params = []\n",
    "for parameter in unique_parameters:\n",
    "    if x[parameter] / n > 10:\n",
    "        median_params.append(parameter)\n",
    "    elif x[parameter] / n > 1:\n",
    "        mean_params.append(parameter)\n",
    "    elif x[parameter] / n < 1:\n",
    "        rare_params.append(parameter)\n",
    "    else:\n",
    "        one_params.append(parameter)\n",
    "        \n",
    "feature_list = one_params + rare_params + mean_params + median_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наполнение структуры данных согласно указанныму выше алгоритму и сохранение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = feature_list, index = range(n))\n",
    "for i, rid in enumerate(RIDs):\n",
    "    df_ = pd.read_table('set/' + str(rid) + '.txt', delimiter = ',')\n",
    "    df_edited = pd.DataFrame(0, index = range(1), columns = feature_list)\n",
    "    for parameter in one_params:\n",
    "        df_edited[parameter] = df_['Value'].where(df_[\"Parameter\"] == parameter).sum()\n",
    "    for parameter in rare_params:\n",
    "        df_edited[parameter] = df_['Value'].where(df_[\"Parameter\"] == parameter).mean()\n",
    "    for parameter in mean_params:\n",
    "        df_edited[parameter] = (df_['Value'].where(df_[\"Parameter\"] == parameter)).mean()\n",
    "    for parameter in median_params: \n",
    "        df_edited[parameter] = (df_['Value'].where(df_[\"Parameter\"] == parameter)).median()\n",
    "    df.loc[i, feature_list] = df_edited.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_table('outcomes.txt', delimiter = ',')\n",
    "df[\"Survival\"] = out[\"Survival\"]\n",
    "df[\"In-hospital_death\"] = out[\"In-hospital_death\"]\n",
    "print(df)\n",
    "df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Описание полученной структуры данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для полей из rare-params и mean-params рассчитывается среднее значение из имеющихся по человеку \n",
    "print(\"rare_params: \", rare_params)\n",
    "print(\"mean_params: \", mean_params)\n",
    "# Для полей из one-params Выдаётся единственное имеющееся значение\n",
    "print(\"one_params: \", one_params)\n",
    "# Для полей из median_params рассчитывается медианное значение\n",
    "print(\"median_params: \", median_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Название полей\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# типы полей\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Устранение дубликатов, пустых записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальная обработка.\n",
    "Удаляем одинаковые столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"] = df[\"Gender\"].replace(-1, np.NaN)\n",
    "df[\"Gender\"] = df[\"Gender\"] > 0\n",
    "df[\"MechVent\"] = df[\"MechVent\"] > 0\n",
    "df = df.drop(columns = [\"ICUType\"])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты отсутствуют, так как каждый файл с пациентом имеет своё уникальное имя, соответствующее RecordID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём проверку на наличие пустых записей в таблице: записей, в которых совсем нет значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, есть ли строчки, в которых есть только обязательные значения (порог 11 элементов) и удалим их, потому что они не помогут в обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(thresh = 11) #Почему именно 11?\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Обработка пропущенных значений, выбросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем все признаки, для которых количество NaN значений превышает 90%, потому что эти столбцы нельзя будет использовать для обучения в дальнейшем (мало данных), а заполнять отсутствующие значения некорректно по 10% имеющимся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = len(df)\n",
    "df[\"Height\"] = df[\"Height\"].replace(-1, np.NaN)\n",
    "df[\"Weight\"] = df[\"Weight\"].replace(-1, np.NaN)\n",
    "\n",
    "print (\"{:<45} {:<10} {:<10}\".format('Характеристика', 'Кол-во NaN', 'Процент NaN')) \n",
    "for column in df.columns: \n",
    "    print(\"{:<50} {:<10} {:<10}\".format(column, df[column].isnull().sum(), round(100 * df[column].isnull().sum() / df_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Cholesterol\", \"TroponinI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем все строчки, для которых процент NaN >= 35%. Значение в 35% выбрали по графику, в котором пациенты отсортированы по количеству отсутствующих значений. При выборе меньшего порога будет отсечено гораздо больше данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((df.isnull().sum(axis = 1)).sort_values().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.isnull().sum(axis = 1) / len(df.columns) < 0.35]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    plt.figure()\n",
    "    df.boxplot([column])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аномалии - значения, которые находятся вне интервала [Q1 - (Q3 - Q1) * 1.5; Q3 + (Q3 - Q1) * 1.5], изображаются точками на графике \"ящик с усами\", но не являются выбросами (см. далее). эти значения оставляем без изменения, так как они могут помочь при обучении.\n",
    "Под выбросами понимаем значения, которые находятся вне интервала [Q1 - (Q3 - Q1) * 5; Q3 + (Q3 - Q1) * 5]. Их заменяем на NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column not in [\"Survival\", \"In-hospital_death\"] and df.dtypes[column] != 'bool':\n",
    "        values = df[column] \n",
    "        q1, q3 = values.quantile([0.25, 0.75])\n",
    "        low  = q1 - (q3 - q1) * 5\n",
    "        high = q3 + (q3 - q1) * 5\n",
    "        condition = (values < low) | (values > high)\n",
    "        values[condition] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Получение описательных статистик и графиков распределения всех признаков из итогового набора полей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описательные статистики итогового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Survival\"][df[\"Survival\"] < -1] = -1 #Обработка значений меньше чем <-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим гистаграммы по значениям в колонках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()\n",
    "figure, axis = plt.subplots((df.dtypes=='float64').sum(), 1)\n",
    "figure.set_size_inches(15, 250)\n",
    "i = 0\n",
    "for column in df.columns:\n",
    "    if df.dtypes[column]=='float64':\n",
    "        s = pd.Series(data[:, df.columns.get_loc(column)])\n",
    "        axis[i].hist(s, bins = 10, log = True)\n",
    "        axis[i].set_title(column)\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, насколько распределение для каждого признака для не-NaN значений является нормальным, логнормальным, экпоненциальным или равномерным, используя библиотеку fitter, в которой подбираются парааметры для каждого из распределений, минимизируя RSS (сумма квадратов ошибок от распределения) по гистограмме из 15 интервалов (при большем количестве интервалов могут проявиться некорректные результаты из-за работы с целыми числами в некоторых признаках, например, рост или возраст, когда в интервал может не попасть ни одного значения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "f = [None] * (df.dtypes=='float64').sum()\n",
    "for column in df.columns:\n",
    "    if df.dtypes[column]=='float64':\n",
    "        f[i] = Fitter(df[column][~df[column].isnull()],\n",
    "                   distributions=[\"expon\", \"uniform\", \"norm\", \"lognorm\"],\n",
    "                   bins = 15)\n",
    "        time.sleep(0.2)\n",
    "        f[i].fit()\n",
    "        print(column)\n",
    "        print(f[i].summary()['sumsquare_error'])\n",
    "        i += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех признаках, кроме указанных ниже, меньшая ошибка была при расчёте для нормального или логнормального распределения. Так как нормальное распределение предполагает возможность отрицательных значений, а логнормальное нет, то между этих двух будем выбирать логнормальное.\n",
    "\n",
    "RecordID - равномерное: логично, так как номера ID указываются в равномерном порядке по нарастанию (с некоторыми пропусками).\n",
    "GCS, GCS_25, GCS_75 - sumsquare_error показывает большую ошибку для любого распределения: по графику видно, что в данных признаках не сформированно хоть какое-то распределение, поэтому предлагается исключить данные признаки из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График для _ПАРАМЕТР_ - пример нормального/логнормального распределения\n",
    "f[3].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График для _ПАРАМЕТР_ - пример экспоненциального/логнормального распределения\n",
    "f[6].summary() #Подставлять число -> номер графика подходящего под пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# График для GCS: пример с неопределённым распределением\n",
    "f[26].summary() # Поставить число неопределнного распределения, если нет такого, строку удалить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем NaN-значения согласно найденным распределениям, т. е. случайными величинами с известными параметрами распределений, для сохранения распределения значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column not in [\"RecordID\"]:\n",
    "        values = df[column] \n",
    "        E = values.mean()\n",
    "        D = values.var()\n",
    "        sigma = np.log(D / (E ** 2) + 1) ** 0.5\n",
    "        mu = np.log(E) - (sigma ** 2) / 2\n",
    "        condition = values.isna()\n",
    "        new_values =  abs(np.random.lognormal(mu, sigma, len(df.index)))\n",
    "        values[condition] = new_values[condition]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка, что NaN больше нет в датасете\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Характер зависимости между признаками можно посмотреть по матрице корреляций: на heatmap цветом изображён коэффициент линейной корреляции между каждой парой признаков. Чем светлее, тем больше прямая зависимость, чем темнее - обратная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 10)) \n",
    "sns.heatmap(corr, ax=ax, xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример-проверка, что NaN удалились в столбце, где изначально было 77% NaN\n",
    "data = df.to_numpy()\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "figure.set_size_inches(15, 5)\n",
    "s = pd.Series(data[:, df.columns.get_loc(\"TroponinT\")])\n",
    "print(len(s), len(df.index))\n",
    "axis.hist(s, bins = 10, log = True)\n",
    "axis.set_title(\"TroponinT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Отбор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем функцию SelectKBest для нахождения k = 20 признаков (треть от имеющихся), имеющих наибольшее влияние на 2 выходные переменные, со следующими критериями:\n",
    "1) Критерий хи-квадрат для классификации (метка 'In-hospital_death')\n",
    "2) Коэффициент линейной корреляции Пирсона для регрессии (метка 'Survival')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_cls = df['In-hospital_death']\n",
    "X_cls = df.drop(columns = ['In-hospital_death', 'Survival'])\n",
    "Y_reg = df['Survival']\n",
    "X_reg = df.drop(columns = ['In-hospital_death', 'Survival'])\n",
    "\n",
    "    # может лучше ничего не убирать??\n",
    "# 20 признаков для классификации\n",
    "selector_cls = SelectKBest(chi2, k = 20)\n",
    "X_new = selector_cls.fit_transform(X_cls*1, Y_cls)\n",
    "features_cls = X_cls.columns[selector_cls.get_support()]\n",
    "print(features_cls)\n",
    "\n",
    "# 20 признаков для регрессии\n",
    "selector_reg = SelectKBest(f_regression, k = 20)\n",
    "X_new = selector_reg.fit_transform(X_reg*1, Y_reg)\n",
    "features_reg = X_reg.columns[selector_reg.get_support()]\n",
    "print(features_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Красным показаны значения p-values для 20 признаков, которые мы выбрали\n",
    "# Синим выделены признаки, которые мы не выбираем\n",
    "\n",
    "mask = selector_cls.get_support()\n",
    "scores = -np.log10(selector_cls.pvalues_)\n",
    "X_indices = np.arange(X_cls.shape[-1])\n",
    "plt.figure(1, figsize=[15, 7])\n",
    "plt.clf()\n",
    "plt.bar(X_indices[mask], scores[mask] ** 0.5, width=0.2, color = 'red')\n",
    "plt.bar(X_indices[~mask], scores[~mask] ** 0.5, width=0.2, color = 'blue')\n",
    "plt.show()\n",
    "X_cls = X_cls.loc[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = selector_reg.get_support()\n",
    "scores = -np.log10(selector_reg.pvalues_)\n",
    "X_indices = np.arange(X_reg.shape[-1])\n",
    "plt.figure(1, figsize=[15, 7])\n",
    "plt.clf()\n",
    "plt.bar(X_indices[mask], scores[mask] ** 0.5, width=0.2, color = 'red')\n",
    "plt.bar(X_indices[~mask], scores[~mask] ** 0.5, width=0.2, color = 'blue')\n",
    "plt.show()\n",
    "X_reg = X_reg.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Подбор и реализация алгоритмов выявления зависимостей параметров данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый алгортим выяснения зависимостей. PCA-анализ (Метод главных компонент)\n",
    "Определяем 2 компоненты в признаковом пространстве, для которых дисперсия значений будет наибольшей.\n",
    "С помощью цвета можно определить разброс признака внутри компонент и таким образом отследить зависимость одного признака от остальных.\n",
    "Показан график для признака 'Age' для регресионного анализа. Внизу представлены сформированные компоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "X = X_reg.to_numpy()\n",
    "X = X.astype(float)\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X\n",
    "X_centered = (X - X.mean(axis=0)) / np.std(X, axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "# p - признак, по которому выводим цвет, синий - ближе к минимальному значению, красный - к максимальному\n",
    "p = 0\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "figure.set_size_inches(10, 10)\n",
    "axis.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=(X[:, p] - np.min(X[:, p])) /  (np.max(X[:, p])- np.min(X[:, p])), cmap='jet')\n",
    "plt.show()\n",
    "\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, X_reg.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "figure.set_size_inches(10, 10)\n",
    "temp = Y_reg.copy()\n",
    "temp[temp == -1] = float(\"+inf\")\n",
    "temp[temp < 2] = 2\n",
    "temp = 1 / np.log2(temp)\n",
    "indx = temp > 0.3\n",
    "axis.scatter(X_pca[indx, 0], X_pca[indx, 1], s = X[indx, p], c = temp[indx], cmap='copper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_cls.to_numpy()\n",
    "X = X.astype(float)\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X\n",
    "X_centered = (X - X.mean(axis=0)) / np.std(X, axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "# p - признак, по которому выводим цвет, синий - ближе к минимальному значению, красный - к максимальному\n",
    "p = 0\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "figure.set_size_inches(10, 10)\n",
    "axis.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=(X[:, p] - np.min(X[:, p])) /  (np.max(X[:, p])- np.min(X[:, p])), cmap='jet')\n",
    "plt.show()\n",
    "\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, X_cls.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "figure, axis = plt.subplots(1, 1)\n",
    "figure.set_size_inches(10, 10)\n",
    "axis.scatter(X_pca[:, 0], X_pca[:, 1], s = 10 + 50*Y_cls, c =(X[:, p] - np.min(X[:, p])) /  (np.max(X[:, p])- np.min(X[:, p])), cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg_corr = X_reg.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 10)) \n",
    "sns.heatmap(X_reg_corr, ax=ax, xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cls_corr = X_cls.corr()\n",
    "fig, ax = plt.subplots(figsize=(15, 10)) \n",
    "sns.heatmap(X_cls_corr, ax=ax, xticklabels=True, yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Разбиение обработанных данных на обучающую, валидационную и тестирующую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sum(Y_cls), len(Y_cls)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дисбаланс для задачи классификации составляет 1090/7683 = 14%. Это означает, что 14% данных относятся к классу \"Умерли в госпитализации\", а оставшиеся к классу \"Не умерли в госпитализации\". По описанию несбалансированных данных Google (https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data) незначительный уровень дисбаланса соответствует доле меньшинства в 20-40% от всего множества данных. Поэтому предлагается удалить некоторое число данных, относящихся к мажоритарному классу, чтобы дисбаланс стал равен 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sum(Y_reg==-1), sum(Y_reg==0), sum(Y_reg==1), sum((Y_reg>=0)*(Y_reg<=360)), sum(Y_reg>360), max(Y_reg), Y_reg.quantile(0.95), len(Y_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_reg.sort_values().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Непоследовательность определения значений для решения задачи регрессии состоит в том, что 4794/7683 = 62% людей имеют поле 'Survival' равным -1, то есть не умерли на момент получения данных, по сути это означает, что эта -1 соответствует +Inf. Преобразовываем поле 'Survival' по следующей формуле, для получения значений, которые можно предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Y_reg.copy()\n",
    "temp[temp == -1] = float(\"+inf\")\n",
    "temp[temp < 2] = 2\n",
    "temp = 1 / np.log2(temp)\n",
    "Y_reg = temp\n",
    "plt.plot(Y_reg.sort_values().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indx_1 = Y_cls == 1\n",
    "indx_0 = Y_cls == 0\n",
    "# Удаляем строчки с мажоритарным классом с вероятностью (1 - sum(Y_cls) / len(Y_cls))\n",
    "counter = 0\n",
    "for i in range(len(Y_cls)):\n",
    "    if indx_0[i] == True:\n",
    "        counter = counter + 1\n",
    "    if counter > 2741:\n",
    "        indx_0[i] = False\n",
    "\n",
    "X_train_cls1, X_test_cls1, y_train_cls1, y_test_cls1 = train_test_split(X_cls[indx_1], Y_cls[indx_1], test_size=0.1, random_state=1)\n",
    "X_train_cls1, X_val_cls1, y_train_cls1, y_val_cls1 = train_test_split(X_train_cls1, y_train_cls1, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train_cls0, X_test_cls0, y_train_cls0, y_test_cls0 = train_test_split(X_cls[indx_0], Y_cls[indx_0], test_size=0.24, random_state=1)\n",
    "X_train_cls0, X_val_cls0, y_train_cls0, y_val_cls0 = train_test_split(X_train_cls0, y_train_cls0, test_size=0.63, random_state=1)\n",
    "\n",
    "X_train_cls = pd.concat([X_train_cls0, X_train_cls1])\n",
    "y_train_cls = pd.concat([y_train_cls0, y_train_cls1])\n",
    "X_val_cls = pd.concat([X_val_cls0, X_val_cls1])\n",
    "y_val_cls = pd.concat([y_val_cls0, y_val_cls1])\n",
    "X_test_cls = pd.concat([X_test_cls0, X_test_cls1])\n",
    "y_test_cls = pd.concat([y_test_cls0, y_test_cls1])\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, Y_reg, test_size=0.1, random_state=1) # 0.2 тестовая выборка\n",
    "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_train_reg, y_train_reg, test_size=0.2, random_state=1) # 0.2*0.8 = 0.18 валидационная, 0.72 обучающая\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Выбор модели прогнозирования и настройка гиперпараметров для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train_cls, y_train_cls)\n",
    "    pred_cls = clf.predict(X_val_cls)\n",
    "    score = f1_score(y_val_cls, pred_cls)\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения лучших алгоритмов бинарной классификации было проведено машинное обучение с использованием основных интеллектуальных моделей на основании 1526 тренировочных данных (783 \"0\" и 783 \"1\", пропорция 1:1 так как данные несбалансированны). Критерий качества (Accuracy) высчитывался на основании 1536 валидационных значений (1318 \"0\" и 218 \"1\" - пропорция как в данных в целом).\n",
    "Пороговое значение для точности равно: (1318/1536)*(1318/1536)+(218/1536)*(218/1536) ~ 0,76. Оно аналогично случайному выбору.\n",
    "\n",
    "Лучшие модели по критерию f1-score: Decision Tree, Random Forest, AdaBoost (аналогичен Random Forest), Linear SVM, Naive Bayes\n",
    "SVM убираем из-за большой вычислительной сложности O(n^2*p+n^3) (n - размер тренировочного набора, p - количество признаков)\n",
    "Random Forest/Decision Tree оставляем, его сложность O(n^2*p)\n",
    "Naive Bayes оставляем, его сложность O(n*p)\n",
    "Neural Net предлагается добавить, её сложность линейно зависит от n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[9, 11, 13], 'n_estimators':[100, 200, 300], 'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "clf = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, scoring='f1')\n",
    "clf.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_cls = clf.predict(X_val_cls)\n",
    "\n",
    "print(precision_score(y_val_cls, pred_cls))\n",
    "print(recall_score(y_val_cls, pred_cls))\n",
    "print(accuracy_score(y_val_cls, pred_cls))\n",
    "print(f1_score(y_val_cls, pred_cls))\n",
    "print(balanced_accuracy_score(y_val_cls, pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cls = clf.predict_proba(X_val_cls)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_val_cls,  pred_cls[:, 1])\n",
    "auc = metrics.roc_auc_score(y_val_cls, pred_cls[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest при настройке гиперпараметров улучшил показатель f1_score с 0.484 (max_depth=5, n_estimators=10, max_features=1) до 0.559 (max_depth=11, n_estimators=100, max_features='sqrt')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes':[(20,), (40,), (100,)], 'activation':['identity', 'relu']}\n",
    "\n",
    "clf = GridSearchCV(estimator=MLPClassifier(), param_grid=parameters, scoring='f1')\n",
    "clf.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_cls = clf.predict(X_val_cls)\n",
    "\n",
    "print(precision_score(y_val_cls, pred_cls))\n",
    "print(recall_score(y_val_cls, pred_cls))\n",
    "print(accuracy_score(y_val_cls, pred_cls))\n",
    "print(f1_score(y_val_cls, pred_cls))\n",
    "print(balanced_accuracy_score(y_val_cls, pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cls = clf.predict_proba(X_val_cls)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_val_cls,  pred_cls[:, 1])\n",
    "auc = metrics.roc_auc_score(y_val_cls, pred_cls[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'var_smoothing':[1e-9, 1e-7, 1e-6, 1e-5]}\n",
    "\n",
    "clf = GridSearchCV(estimator=GaussianNB(), param_grid=parameters, scoring='f1')\n",
    "clf.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_cls = clf.predict(X_val_cls)\n",
    "\n",
    "print(precision_score(y_val_cls, pred_cls))\n",
    "print(recall_score(y_val_cls, pred_cls))\n",
    "print(accuracy_score(y_val_cls, pred_cls))\n",
    "print(f1_score(y_val_cls, pred_cls))\n",
    "print(balanced_accuracy_score(y_val_cls, pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cls = clf.predict_proba(X_val_cls)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_val_cls,  pred_cls[:, 1])\n",
    "auc = metrics.roc_auc_score(y_val_cls, pred_cls[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Прогноз значения целевой переменной, качество модели для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=9, n_estimators=300, max_features='log2')\n",
    "clf.fit(X_train_cls, y_train_cls)\n",
    "pred_cls = clf.predict(X_test_cls)\n",
    "\n",
    "print(precision_score(y_test_cls, pred_cls))\n",
    "print(recall_score(y_test_cls, pred_cls))\n",
    "print(accuracy_score(y_test_cls, pred_cls))\n",
    "print(f1_score(y_test_cls, pred_cls))\n",
    "print(balanced_accuracy_score(y_test_cls, pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cls = clf.predict_proba(X_val_cls)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_val_cls,  pred_cls[:, 1])\n",
    "auc = metrics.roc_auc_score(y_val_cls, pred_cls[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Выбор модели прогнозирования и настройка гиперпараметров для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, max_error\n",
    "from sklearn import metrics\n",
    "\n",
    "names = [\n",
    "    \"LinearRegression\",\n",
    "    \"Ridge\",\n",
    "    \"BayesianRidge\",\n",
    "    \"PolynomialFeatures\",\n",
    "    \"SVR linear\",\n",
    "    \"SVR rbf\",\n",
    "    \"KNeighborsRegressor\",\n",
    "    \"GaussianProcessRegressor\",\n",
    "    \"DecisionTreeRegressor\",\n",
    "    \"RandomForestRegressor\",\n",
    "    \"MLPRegressor\",\n",
    "]\n",
    "\n",
    "regressors = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0, solver='auto'),\n",
    "    BayesianRidge(),\n",
    "    Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression(fit_intercept=False))]),\n",
    "    SVR(kernel='linear'),\n",
    "    SVR(kernel='rbf'),\n",
    "    KNeighborsRegressor(3),\n",
    "    GaussianProcessRegressor(kernel=DotProduct() + WhiteKernel()),\n",
    "    DecisionTreeRegressor(max_depth=5),\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=10, max_features=2),\n",
    "    MLPRegressor(alpha=1, max_iter=1000),\n",
    "]\n",
    "\n",
    "for name, rgr in zip(names, regressors):\n",
    "    rgr.fit(X_train_reg, y_train_reg)\n",
    "    pred_reg = rgr.predict(X_val_reg)\n",
    "    score = mean_squared_error(y_val_reg, pred_reg)\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пороговое значение MSE, когда в качестве y выбираем среднее значение по y\n",
    "np.mean((np.mean(y_val_reg)-y_val_reg) **2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем следующие модели для подбора гиперпараметров:\n",
    "LinearRegression (без настройки)\n",
    "BayesianRidge \n",
    "KNeighborsRegressor \n",
    "DecisionTreeRegressor\n",
    "RandomForestRegressor \n",
    "MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[13], 'n_estimators':[200], 'max_features': ['sqrt']}\n",
    "\n",
    "rgr = GridSearchCV(estimator=RandomForestRegressor(), param_grid=parameters, scoring='neg_mean_squared_error')\n",
    "rgr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "results = pd.DataFrame.from_dict(rgr.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_reg = rgr.predict(X_val_reg)\n",
    "\n",
    "print(mean_squared_error(y_val_reg, pred_reg))\n",
    "print(r2_score(y_val_reg, pred_reg))\n",
    "print(mean_absolute_error(y_val_reg, pred_reg))\n",
    "print(max_error(y_val_reg, pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[5, 10, 15], 'max_features': [1, 2, 'log2', 'sqrt']}\n",
    "\n",
    "rgr = GridSearchCV(estimator=DecisionTreeRegressor(), param_grid=parameters, scoring='neg_mean_squared_error')\n",
    "rgr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "results = pd.DataFrame.from_dict(rgr.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_reg = rgr.predict(X_val_reg)\n",
    "\n",
    "print(mean_squared_error(y_val_reg, pred_reg))\n",
    "print(r2_score(y_val_reg, pred_reg))\n",
    "print(mean_absolute_error(y_val_reg, pred_reg))\n",
    "print(max_error(y_val_reg, pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':[15, 25, 50], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
    "\n",
    "rgr = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=parameters, scoring='neg_mean_squared_error')\n",
    "rgr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "results = pd.DataFrame.from_dict(rgr.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_reg = rgr.predict(X_val_reg)\n",
    "\n",
    "print(mean_squared_error(y_val_reg, pred_reg))\n",
    "print(r2_score(y_val_reg, pred_reg))\n",
    "print(mean_absolute_error(y_val_reg, pred_reg))\n",
    "print(max_error(y_val_reg, pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_iter':[500], 'tol':[1e-4], 'alpha_1':[1e-5, 1e-6, 1e-7], 'alpha_2':[1e-5, 1e-6, 1e-7]}\n",
    "\n",
    "rgr = GridSearchCV(estimator=BayesianRidge(), param_grid=parameters, scoring='neg_mean_squared_error')\n",
    "rgr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "results = pd.DataFrame.from_dict(rgr.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_reg = rgr.predict(X_val_reg)\n",
    "\n",
    "print(mean_squared_error(y_val_reg, pred_reg))\n",
    "print(r2_score(y_val_reg, pred_reg))\n",
    "print(mean_absolute_error(y_val_reg, pred_reg))\n",
    "print(max_error(y_val_reg, pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes':[(20, ), (50, ), (100, )], 'activation':['relu', 'logistic'], 'solver':['adam', 'lbfgs']}\n",
    "\n",
    "rgr = GridSearchCV(estimator=MLPRegressor(), param_grid=parameters, scoring='neg_mean_squared_error')\n",
    "rgr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "results = pd.DataFrame.from_dict(rgr.cv_results_)\n",
    "results = results.sort_values('rank_test_score', ignore_index=True)\n",
    "print(results['params'][0])\n",
    "pred_reg = rgr.predict(X_val_reg)\n",
    "\n",
    "print(mean_squared_error(y_val_reg, pred_reg))\n",
    "print(r2_score(y_val_reg, pred_reg))\n",
    "print(mean_absolute_error(y_val_reg, pred_reg))\n",
    "print(max_error(y_val_reg, pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
